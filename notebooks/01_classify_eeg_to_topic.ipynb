{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trials: 238\n",
      "Train+Val trials: 215\n",
      "Test trials: 23\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 1: Loss = 2.5807\n",
      "Epoch 2: Loss = 2.3282\n",
      "Epoch 3: Loss = 2.2707\n",
      "Epoch 4: Loss = 2.1522\n",
      "Epoch 5: Loss = 2.0547\n",
      "Epoch 6: Loss = 1.9401\n",
      "Epoch 7: Loss = 1.9316\n",
      "Epoch 8: Loss = 1.8611\n",
      "Epoch 9: Loss = 1.8223\n",
      "Epoch 10: Loss = 1.7084\n",
      "Epoch 11: Loss = 1.7350\n",
      "Epoch 12: Loss = 1.6153\n",
      "Epoch 13: Loss = 1.5035\n",
      "Epoch 14: Loss = 1.5304\n",
      "Epoch 15: Loss = 1.5206\n",
      "Epoch 16: Loss = 1.3805\n",
      "Epoch 17: Loss = 1.4087\n",
      "Epoch 18: Loss = 1.3584\n",
      "Epoch 19: Loss = 1.2612\n",
      "Epoch 20: Loss = 1.3253\n",
      "Epoch 21: Loss = 1.2390\n",
      "Epoch 22: Loss = 1.2172\n",
      "Epoch 23: Loss = 1.2245\n",
      "Epoch 24: Loss = 1.1129\n",
      "Epoch 25: Loss = 1.1231\n",
      "Epoch 26: Loss = 1.0497\n",
      "Epoch 27: Loss = 1.0459\n",
      "Epoch 28: Loss = 1.0587\n",
      "Epoch 29: Loss = 1.0105\n",
      "Epoch 30: Loss = 0.9775\n",
      "Epoch 31: Loss = 0.9297\n",
      "Epoch 32: Loss = 0.9398\n",
      "Epoch 33: Loss = 0.9117\n",
      "Epoch 34: Loss = 0.9705\n",
      "Epoch 35: Loss = 0.9490\n",
      "Epoch 36: Loss = 0.8895\n",
      "Epoch 37: Loss = 0.8100\n",
      "Epoch 38: Loss = 0.8075\n",
      "Epoch 39: Loss = 0.7741\n",
      "Epoch 40: Loss = 0.7760\n",
      "Epoch 41: Loss = 0.8236\n",
      "Epoch 42: Loss = 0.7607\n",
      "Epoch 43: Loss = 0.7729\n",
      "Epoch 44: Loss = 0.7544\n",
      "Epoch 45: Loss = 0.8023\n",
      "Epoch 46: Loss = 0.7674\n",
      "Epoch 47: Loss = 0.6904\n",
      "Epoch 48: Loss = 0.6902\n",
      "Epoch 49: Loss = 0.6757\n",
      "Epoch 50: Loss = 0.6565\n",
      "Epoch 51: Loss = 0.6530\n",
      "Epoch 52: Loss = 0.6994\n",
      "Epoch 53: Loss = 0.5826\n",
      "Epoch 54: Loss = 0.6504\n",
      "Epoch 55: Loss = 0.6171\n",
      "Epoch 56: Loss = 0.6274\n",
      "Epoch 57: Loss = 0.6379\n",
      "Epoch 58: Loss = 0.5622\n",
      "Epoch 59: Loss = 0.5854\n",
      "Epoch 60: Loss = 0.5708\n",
      "Epoch 61: Loss = 0.5798\n",
      "Epoch 62: Loss = 0.5529\n",
      "Epoch 63: Loss = 0.5740\n",
      "Epoch 64: Loss = 0.5339\n",
      "Epoch 65: Loss = 0.5182\n",
      "Epoch 66: Loss = 0.5266\n",
      "Epoch 67: Loss = 0.5383\n",
      "Epoch 68: Loss = 0.5740\n",
      "Epoch 69: Loss = 0.5164\n",
      "Epoch 70: Loss = 0.4787\n",
      "Epoch 71: Loss = 0.4700\n",
      "Epoch 72: Loss = 0.4678\n",
      "Epoch 73: Loss = 0.5039\n",
      "Epoch 74: Loss = 0.4547\n",
      "Epoch 75: Loss = 0.4422\n",
      "Epoch 76: Loss = 0.4678\n",
      "Epoch 77: Loss = 0.4398\n",
      "Epoch 78: Loss = 0.4639\n",
      "Epoch 79: Loss = 0.4299\n",
      "Epoch 80: Loss = 0.4337\n",
      "Epoch 81: Loss = 0.4031\n",
      "Epoch 82: Loss = 0.3924\n",
      "Epoch 83: Loss = 0.4617\n",
      "Epoch 84: Loss = 0.4251\n",
      "Epoch 85: Loss = 0.4220\n",
      "Epoch 86: Loss = 0.4512\n",
      "Epoch 87: Loss = 0.4128\n",
      "Epoch 88: Loss = 0.3862\n",
      "Epoch 89: Loss = 0.4016\n",
      "Epoch 90: Loss = 0.3731\n",
      "Epoch 91: Loss = 0.3733\n",
      "Epoch 92: Loss = 0.4138\n",
      "Epoch 93: Loss = 0.3545\n",
      "Epoch 94: Loss = 0.3796\n",
      "Epoch 95: Loss = 0.3946\n",
      "Epoch 96: Loss = 0.3868\n",
      "Epoch 97: Loss = 0.3624\n",
      "Epoch 98: Loss = 0.3597\n",
      "Epoch 99: Loss = 0.3410\n",
      "Epoch 100: Loss = 0.3505\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 1: Loss = 2.5269\n",
      "Epoch 2: Loss = 2.3498\n",
      "Epoch 3: Loss = 2.2524\n",
      "Epoch 4: Loss = 2.1719\n",
      "Epoch 5: Loss = 2.1096\n",
      "Epoch 6: Loss = 2.0213\n",
      "Epoch 7: Loss = 1.9563\n",
      "Epoch 8: Loss = 1.9101\n",
      "Epoch 9: Loss = 1.8451\n",
      "Epoch 10: Loss = 1.7844\n",
      "Epoch 11: Loss = 1.7231\n",
      "Epoch 12: Loss = 1.6680\n",
      "Epoch 13: Loss = 1.6176\n",
      "Epoch 14: Loss = 1.5480\n",
      "Epoch 15: Loss = 1.5470\n",
      "Epoch 16: Loss = 1.4820\n",
      "Epoch 17: Loss = 1.4313\n",
      "Epoch 18: Loss = 1.3991\n",
      "Epoch 19: Loss = 1.3674\n",
      "Epoch 20: Loss = 1.3273\n",
      "Epoch 21: Loss = 1.2928\n",
      "Epoch 22: Loss = 1.2536\n",
      "Epoch 23: Loss = 1.2393\n",
      "Epoch 24: Loss = 1.1864\n",
      "Epoch 25: Loss = 1.1430\n",
      "Epoch 26: Loss = 1.1412\n",
      "Epoch 27: Loss = 1.1105\n",
      "Epoch 28: Loss = 1.0801\n",
      "Epoch 29: Loss = 1.0733\n",
      "Epoch 30: Loss = 1.0314\n",
      "Epoch 31: Loss = 1.0365\n",
      "Epoch 32: Loss = 0.9562\n",
      "Epoch 33: Loss = 0.9774\n",
      "Epoch 34: Loss = 0.9792\n",
      "Epoch 35: Loss = 0.9239\n",
      "Epoch 36: Loss = 0.9071\n",
      "Epoch 37: Loss = 0.9363\n",
      "Epoch 38: Loss = 0.8986\n",
      "Epoch 39: Loss = 0.8971\n",
      "Epoch 40: Loss = 0.8368\n",
      "Epoch 41: Loss = 0.8429\n",
      "Epoch 42: Loss = 0.8146\n",
      "Epoch 43: Loss = 0.7955\n",
      "Epoch 44: Loss = 0.7875\n",
      "Epoch 45: Loss = 0.7783\n",
      "Epoch 46: Loss = 0.7588\n",
      "Epoch 47: Loss = 0.7409\n",
      "Epoch 48: Loss = 0.7154\n",
      "Epoch 49: Loss = 0.7378\n",
      "Epoch 50: Loss = 0.7101\n",
      "Epoch 51: Loss = 0.6920\n",
      "Epoch 52: Loss = 0.6865\n",
      "Epoch 53: Loss = 0.6958\n",
      "Epoch 54: Loss = 0.6806\n",
      "Epoch 55: Loss = 0.6589\n",
      "Epoch 56: Loss = 0.6536\n",
      "Epoch 57: Loss = 0.6654\n",
      "Epoch 58: Loss = 0.6279\n",
      "Epoch 59: Loss = 0.6366\n",
      "Epoch 60: Loss = 0.6096\n",
      "Epoch 61: Loss = 0.5976\n",
      "Epoch 62: Loss = 0.5805\n",
      "Epoch 63: Loss = 0.5661\n",
      "Epoch 64: Loss = 0.5924\n",
      "Epoch 65: Loss = 0.5864\n",
      "Epoch 66: Loss = 0.5555\n",
      "Epoch 67: Loss = 0.5696\n",
      "Epoch 68: Loss = 0.5581\n",
      "Epoch 69: Loss = 0.5361\n",
      "Epoch 70: Loss = 0.5282\n",
      "Epoch 71: Loss = 0.5308\n",
      "Epoch 72: Loss = 0.5223\n",
      "Epoch 73: Loss = 0.5314\n",
      "Epoch 74: Loss = 0.5199\n",
      "Epoch 75: Loss = 0.5187\n",
      "Epoch 76: Loss = 0.4989\n",
      "Epoch 77: Loss = 0.4803\n",
      "Epoch 78: Loss = 0.4756\n",
      "Epoch 79: Loss = 0.4822\n",
      "Epoch 80: Loss = 0.5018\n",
      "Epoch 81: Loss = 0.4794\n",
      "Epoch 82: Loss = 0.4718\n",
      "Epoch 83: Loss = 0.4553\n",
      "Epoch 84: Loss = 0.4380\n",
      "Epoch 85: Loss = 0.4465\n",
      "Epoch 86: Loss = 0.4757\n",
      "Epoch 87: Loss = 0.4264\n",
      "Epoch 88: Loss = 0.4457\n",
      "Epoch 89: Loss = 0.4164\n",
      "Epoch 90: Loss = 0.4338\n",
      "Epoch 91: Loss = 0.4053\n",
      "Epoch 92: Loss = 0.4296\n",
      "Epoch 93: Loss = 0.4314\n",
      "Epoch 94: Loss = 0.4273\n",
      "Epoch 95: Loss = 0.3932\n",
      "Epoch 96: Loss = 0.4109\n",
      "Epoch 97: Loss = 0.4072\n",
      "Epoch 98: Loss = 0.3641\n",
      "Epoch 99: Loss = 0.3794\n",
      "Epoch 100: Loss = 0.3849\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 1: Loss = 2.5379\n",
      "Epoch 2: Loss = 2.3330\n",
      "Epoch 3: Loss = 2.2224\n",
      "Epoch 4: Loss = 2.1336\n",
      "Epoch 5: Loss = 2.0451\n",
      "Epoch 6: Loss = 1.9692\n",
      "Epoch 7: Loss = 1.8791\n",
      "Epoch 8: Loss = 1.8249\n",
      "Epoch 9: Loss = 1.7324\n",
      "Epoch 10: Loss = 1.6866\n",
      "Epoch 11: Loss = 1.6238\n",
      "Epoch 12: Loss = 1.5684\n",
      "Epoch 13: Loss = 1.5009\n",
      "Epoch 14: Loss = 1.4368\n",
      "Epoch 15: Loss = 1.3993\n",
      "Epoch 16: Loss = 1.3700\n",
      "Epoch 17: Loss = 1.3325\n",
      "Epoch 18: Loss = 1.2669\n",
      "Epoch 19: Loss = 1.2385\n",
      "Epoch 20: Loss = 1.2039\n",
      "Epoch 21: Loss = 1.1490\n",
      "Epoch 22: Loss = 1.1489\n",
      "Epoch 23: Loss = 1.0955\n",
      "Epoch 24: Loss = 1.0782\n",
      "Epoch 25: Loss = 1.0735\n",
      "Epoch 26: Loss = 1.0255\n",
      "Epoch 27: Loss = 1.0293\n",
      "Epoch 28: Loss = 0.9759\n",
      "Epoch 29: Loss = 0.9515\n",
      "Epoch 30: Loss = 0.9369\n",
      "Epoch 31: Loss = 0.9331\n",
      "Epoch 32: Loss = 0.9242\n",
      "Epoch 33: Loss = 0.8879\n",
      "Epoch 34: Loss = 0.8795\n",
      "Epoch 35: Loss = 0.8597\n",
      "Epoch 36: Loss = 0.8512\n",
      "Epoch 37: Loss = 0.8303\n",
      "Epoch 38: Loss = 0.7926\n",
      "Epoch 39: Loss = 0.7699\n",
      "Epoch 40: Loss = 0.7726\n",
      "Epoch 41: Loss = 0.7296\n",
      "Epoch 42: Loss = 0.7379\n",
      "Epoch 43: Loss = 0.7268\n",
      "Epoch 44: Loss = 0.7202\n",
      "Epoch 45: Loss = 0.7202\n",
      "Epoch 46: Loss = 0.6974\n",
      "Epoch 47: Loss = 0.6979\n",
      "Epoch 48: Loss = 0.6592\n",
      "Epoch 49: Loss = 0.6592\n",
      "Epoch 50: Loss = 0.7175\n",
      "Epoch 51: Loss = 0.6592\n",
      "Epoch 52: Loss = 0.6318\n",
      "Epoch 53: Loss = 0.6715\n",
      "Epoch 54: Loss = 0.6072\n",
      "Epoch 55: Loss = 0.6199\n",
      "Epoch 56: Loss = 0.6108\n",
      "Epoch 57: Loss = 0.5807\n",
      "Epoch 58: Loss = 0.5962\n",
      "Epoch 59: Loss = 0.5781\n",
      "Epoch 60: Loss = 0.5649\n",
      "Epoch 61: Loss = 0.5480\n",
      "Epoch 62: Loss = 0.5564\n",
      "Epoch 63: Loss = 0.5336\n",
      "Epoch 64: Loss = 0.5085\n",
      "Epoch 65: Loss = 0.5030\n",
      "Epoch 66: Loss = 0.5157\n",
      "Epoch 67: Loss = 0.5382\n",
      "Epoch 68: Loss = 0.5345\n",
      "Epoch 69: Loss = 0.4969\n",
      "Epoch 70: Loss = 0.5138\n",
      "Epoch 71: Loss = 0.4942\n",
      "Epoch 72: Loss = 0.4572\n",
      "Epoch 73: Loss = 0.4934\n",
      "Epoch 74: Loss = 0.4677\n",
      "Epoch 75: Loss = 0.4740\n",
      "Epoch 76: Loss = 0.4400\n",
      "Epoch 77: Loss = 0.4358\n",
      "Epoch 78: Loss = 0.4479\n",
      "Epoch 79: Loss = 0.4400\n",
      "Epoch 80: Loss = 0.4219\n",
      "Epoch 81: Loss = 0.4430\n",
      "Epoch 82: Loss = 0.4286\n",
      "Epoch 83: Loss = 0.4126\n",
      "Epoch 84: Loss = 0.4258\n",
      "Epoch 85: Loss = 0.4133\n",
      "Epoch 86: Loss = 0.4430\n",
      "Epoch 87: Loss = 0.3917\n",
      "Epoch 88: Loss = 0.3825\n",
      "Epoch 89: Loss = 0.3730\n",
      "Epoch 90: Loss = 0.3828\n",
      "Epoch 91: Loss = 0.3846\n",
      "Epoch 92: Loss = 0.3559\n",
      "Epoch 93: Loss = 0.3616\n",
      "Epoch 94: Loss = 0.3973\n",
      "Epoch 95: Loss = 0.3621\n",
      "Epoch 96: Loss = 0.3612\n",
      "Epoch 97: Loss = 0.3559\n",
      "Epoch 98: Loss = 0.3575\n",
      "Epoch 99: Loss = 0.3410\n",
      "Epoch 100: Loss = 0.3412\n",
      "\n",
      "Cross-validation results:\n",
      "   fold  accuracy        f1\n",
      "0     1  0.043478  0.069565\n",
      "1     2  0.173913  0.202899\n",
      "2     3  0.130435  0.157350\n",
      "\n",
      "Mean Accuracy: 0.11594202898550725\n",
      "Mean F1 Score: 0.14327122153209107\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import uuid\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from src.utils.feature_cleaner import clean_features\n",
    "from src.utils.feature_selector import select_features\n",
    "from src.models.matching_model import get_model\n",
    "from src.utils.trial_split import prepare_splits\n",
    "from src.train.model_configs import MODEL_CONFIGS, CLASSIC_MODEL_CONFIGS\n",
    "from src.utils.label_maps import TOPIC_TO_META\n",
    "from src.utils.balance_topic_trials import balance_trials_by_topic\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ───────────────────────────────────────────────────────────── #\n",
    "# Full Notebook Code: EEG → Topic Classification (MLP + Classic)\n",
    "# ───────────────────────────────────────────────────────────── #\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "PREFIX = \"/absolute/or/relative/project_root/\"   # <- change & keep trailing slash\n",
    "DATA_PATH = f\"{PREFIX}data/clean_full_dataset_with_avg_epochs.csv\"\n",
    "LABEL_PATH = f\"{PREFIX}data/merged_embedded_reports.csv\"\n",
    "SAVE_DIR = Path(f\"{PREFIX}notebooks/demo_outputs/\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LABEL_ENCODER_PATH = f\"{PREFIX}notebooks/demo_outputs/topic_label_encoder.pkl\"\n",
    "SCALER_SAVE_PATH = f\"{PREFIX}notebooks/demo_outputs/matching_scaler.pkl\"\n",
    "\n",
    "\n",
    "MODEL_TYPE = \"lightgbm_tuned\"  # or \"mlp\", rf_default, xgb_default ...\n",
    "FEATURE_GROUPS = [\"rel_theta\"] # EEG Feature groups\n",
    "LABEL_COLUMN = \"topic_label\"\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "EPOCH = 1 # or avg01, [1,2] ...\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ========== LOAD & CLEAN ==========\n",
    "eeg_df = pd.read_csv(DATA_PATH)\n",
    "label_df = pd.read_csv(LABEL_PATH)\n",
    "label_df[\"topic_label\"] = label_df[\"Meta Topic ID\"].map(TOPIC_TO_META)\n",
    "label_df[\"trial_name\"] = label_df[\"trial_name\"].str.replace(\".edf\", \"\", regex=False)\n",
    "\n",
    "eeg_df = clean_features(eeg_df)\n",
    "\n",
    "if isinstance(EPOCH, str) and EPOCH.startswith(\"avg\"):\n",
    "    eeg_df = eeg_df[eeg_df[\"epoch\"] == EPOCH]\n",
    "else:\n",
    "    eeg_df[\"epoch\"] = pd.to_numeric(eeg_df[\"epoch\"], errors=\"coerce\")\n",
    "    eeg_df = eeg_df[eeg_df[\"epoch\"].isin([EPOCH])]\n",
    "\n",
    "eeg_df = eeg_df.reset_index(drop=True)\n",
    "eeg_df = eeg_df.merge(label_df[[\"trial_name\", \"Meta Topic ID\"]], on=\"trial_name\", how=\"left\")\n",
    "eeg_df[\"topic_label\"] = eeg_df[\"Meta Topic ID\"].map(TOPIC_TO_META)\n",
    "\n",
    "# ========== SPLIT ==========\n",
    "train_df_full, val_folds, test_df = prepare_splits(eeg_df, trial_column=\"trial_name\", test_size=0.1, seed=SEED)\n",
    "\n",
    "print(f\"Total unique trials: {eeg_df['trial_name'].nunique()}\")\n",
    "print(f\"Train+Val trials: {train_df_full['trial_name'].nunique()}\")\n",
    "print(f\"Test trials: {test_df['trial_name'].nunique()}\")\n",
    "\n",
    "test_trial_names = test_df[\"trial_name\"].unique()\n",
    "with open(SAVE_DIR / \"test_trials.txt\", \"w\") as f:\n",
    "    for trial in test_trial_names:\n",
    "        f.write(trial + \"\\n\")\n",
    "\n",
    "results = []\n",
    "experiment_ids = []\n",
    "base_experiment_id = str(uuid.uuid4())\n",
    "\n",
    "# ========== CROSS-VALIDATION LOOP ==========\n",
    "for fold_idx, val_df in enumerate(val_folds):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "    experiment_id = f\"{base_experiment_id}_fold{fold_idx + 1}\"\n",
    "    experiment_ids.append(experiment_id)\n",
    "\n",
    "    val_trial_names = val_df[\"trial_name\"].unique()\n",
    "    with open(SAVE_DIR / f\"val_trials_fold_{fold_idx + 1}.txt\", \"w\") as f:\n",
    "        for trial in val_trial_names:\n",
    "            f.write(trial + \"\\n\")\n",
    "\n",
    "    train_df = train_df_full[~train_df_full[\"trial_name\"].isin(val_trial_names)].reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    train_df = balance_trials_by_topic(train_df, topic_col=LABEL_COLUMN)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_df[LABEL_COLUMN])\n",
    "\n",
    "    # Select features\n",
    "    X_train, y_train = select_features(train_df, FEATURE_GROUPS, None, LABEL_COLUMN, None)\n",
    "    X_val, y_val = select_features(val_df, FEATURE_GROUPS, None, LABEL_COLUMN, None)\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "    # Save encoder + scaler\n",
    "    joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "\n",
    "    if MODEL_TYPE == \"mlp\":\n",
    "        model_config = MODEL_CONFIGS[\"simple_512\"]\n",
    "        model = get_model(\"mlp\", model_config, input_dim=X_train.shape[1], output_dim=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}: Loss = {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), SAVE_DIR / f\"model_fold{fold_idx + 1}.pt\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_val = model(X_val_tensor.to(device)).argmax(dim=1).cpu().numpy()\n",
    "    else:\n",
    "        config = CLASSIC_MODEL_CONFIGS[MODEL_TYPE]\n",
    "        model = get_model(model_type=config[\"model_type\"], model_config=config, task_type=\"classification\")\n",
    "        model.fit(X_train, y_train_encoded)\n",
    "\n",
    "        # Save model\n",
    "        joblib.dump(model, SAVE_DIR / f\"model_fold{fold_idx + 1}.pt\")\n",
    "\n",
    "        preds_val = model.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val_encoded, preds_val)\n",
    "    f1 = f1_score(y_val_encoded, preds_val, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_val_encoded, preds_val)\n",
    "\n",
    "    unique_labels = np.unique(np.concatenate([y_val_encoded, preds_val]))\n",
    "    class_names = label_encoder.inverse_transform(unique_labels)\n",
    "\n",
    "    cm = confusion_matrix(y_val_encoded, preds_val, labels=unique_labels)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {fold_idx}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(SAVE_DIR / f\"fold_{fold_idx + 1}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    with open(SAVE_DIR / f\"fold_{fold_idx + 1}_predictions.txt\", \"w\") as f:\n",
    "        for true, pred in zip(label_encoder.inverse_transform(y_val_encoded), label_encoder.inverse_transform(preds_val)):\n",
    "            f.write(f\"{true}\\t{pred}\\n\")\n",
    "\n",
    "    results.append({\"fold\": fold_idx + 1, \"accuracy\": acc, \"f1\": f1})\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(df_results)\n",
    "print(\"\\nMean Accuracy:\", df_results[\"accuracy\"].mean())\n",
    "print(\"Mean F1 Score:\", df_results[\"f1\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test samples after filtering: 23\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/seifelhadidi/Desktop/UNI/Thesis/eeg-dream-decoding-clean/notebooks/demo_outputs/model_fold0.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m config \u001b[38;5;241m=\u001b[39m CLASSIC_MODEL_CONFIGS[MODEL_TYPE]\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(model_type\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m], model_config\u001b[38;5;241m=\u001b[39mconfig, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_WEIGHTS_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[1;32m     80\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_scaled)\n",
      "File \u001b[0;32m~/Desktop/UNI/Thesis/eeg-dream-decoding/src/models/matching_model.py:138\u001b[0m, in \u001b[0;36mClassicModelWrapper.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eeg-py39/lib/python3.9/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/seifelhadidi/Desktop/UNI/Thesis/eeg-dream-decoding-clean/notebooks/demo_outputs/model_fold0.pt'"
     ]
    }
   ],
   "source": [
    "# ========== FINAL TEST-SET EVALUATION ==========\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "\n",
    "from src.utils.feature_cleaner   import clean_features\n",
    "from src.utils.feature_selector  import select_features\n",
    "from src.models.matching_model   import get_model\n",
    "from src.utils.label_maps        import TOPIC_TO_META\n",
    "from src.train.model_configs     import MODEL_CONFIGS, CLASSIC_MODEL_CONFIGS\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TEST_TRIALS_PATH    = f\"{PREFIX}notebooks/demo_outputs/test_trials.txt\"\n",
    "\n",
    "BEST_FOLD           = 2\n",
    "MODEL_WEIGHTS_PATH  = f\"{PREFIX}notebooks/demo_outputs/model_fold{BEST_FOLD}.pt\"\n",
    "\n",
    "MODEL_TYPE          = \"lightgbm_tuned\"   # or \"mlp\"\n",
    "FEATURE_GROUPS      = [\"rel_theta\"]\n",
    "SLEEP_STAGE_FILTER  = None\n",
    "EPOCH               = 1\n",
    "DEVICE              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------- LOAD TEST TRIALS ----------------\n",
    "with open(TEST_TRIALS_PATH) as f:\n",
    "    test_trials = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ---------------- LOAD & CLEAN DATA ----------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "labels = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "df[\"trial_name\"]     = df[\"trial_name\"].str.replace(\".edf\", \"\", regex=False)\n",
    "labels[\"trial_name\"] = labels[\"trial_name\"].str.replace(\".edf\", \"\", regex=False)\n",
    "labels[\"topic_label\"] = labels[\"Meta Topic ID\"].map(TOPIC_TO_META)\n",
    "\n",
    "test_df = df[df[\"trial_name\"].isin(test_trials)]\n",
    "test_df = test_df.merge(labels[[\"trial_name\", \"topic_label\"]], on=\"trial_name\", how=\"left\")\n",
    "\n",
    "test_df = clean_features(test_df)\n",
    "if isinstance(EPOCH, str) and EPOCH.startswith(\"avg\"):\n",
    "    test_df = test_df[test_df[\"epoch\"] == EPOCH]\n",
    "else:\n",
    "    test_df[\"epoch\"] = pd.to_numeric(test_df[\"epoch\"], errors=\"coerce\")\n",
    "    test_df = test_df[test_df[\"epoch\"].isin([EPOCH])]\n",
    "\n",
    "\n",
    "# ---------------- SELECT FEATURES ----------------\n",
    "X_test, y_test_series = select_features(\n",
    "    test_df,\n",
    "    group_names           = FEATURE_GROUPS,\n",
    "    sleep_stage           = SLEEP_STAGE_FILTER,\n",
    "    label_df              = None,\n",
    "    target_column         = \"topic_label\",\n",
    "    include_region_summary=False\n",
    ")\n",
    "\n",
    "# ---------------- SCALE + ENCODE ----------------\n",
    "scaler = joblib.load(SCALER_SAVE_PATH)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "valid_mask = y_test_series.isin(label_encoder.classes_)\n",
    "X_test_scaled = X_test_scaled[valid_mask.values]\n",
    "y_test_enc = label_encoder.transform(y_test_series[valid_mask])\n",
    "\n",
    "print(f\"[Info] Test samples after filtering: {len(y_test_enc)}\")\n",
    "\n",
    "# ---------------- LOAD & PREDICT ----------------\n",
    "if MODEL_TYPE in CLASSIC_MODEL_CONFIGS:\n",
    "    config = CLASSIC_MODEL_CONFIGS[MODEL_TYPE]\n",
    "    model = get_model(model_type=config[\"model_type\"], model_config=config, task_type=\"classification\")\n",
    "    model.load(MODEL_WEIGHTS_PATH)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)\n",
    "\n",
    "else:  # MLP\n",
    "    config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "    model = get_model(\n",
    "        model_type   = MODEL_TYPE,\n",
    "        model_config = config,\n",
    "        input_dim    = X_test_scaled.shape[1],\n",
    "        output_dim   = len(label_encoder.classes_)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_test_scaled, dtype=torch.float32).to(DEVICE))\n",
    "        y_prob = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        y_pred = y_prob.argmax(axis=1)\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "acc   = accuracy_score(y_test_enc, y_pred)\n",
    "f1w   = f1_score(y_test_enc, y_pred, average=\"weighted\")\n",
    "lloss = log_loss(y_test_enc, y_prob, labels=range(len(label_encoder.classes_)))\n",
    "\n",
    "print(\"\\n───────── TEST RESULTS ─────────\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"F1-score : {f1w:.4f}\")\n",
    "print(f\"LogLoss  : {lloss:.4f}\")\n",
    "\n",
    "# ---------------- SAVE PREDICTIONS ----------------\n",
    "test_out_df = pd.DataFrame({\n",
    "    \"trial_name\" : test_df[\"trial_name\"].values[valid_mask.values],\n",
    "    \"true_label\" : label_encoder.inverse_transform(y_test_enc),\n",
    "    \"pred_label\" : label_encoder.inverse_transform(y_pred)\n",
    "})\n",
    "test_out_df.to_csv(f\"{PREFIX}notebooks/demo_outputs/test_predictions.csv\", index=False)\n",
    "print(\"[Info] Saved → notebooks/demo_outputs/test_predictions.csv\")\n",
    "\n",
    "# ---------------- CONFUSION MATRIX ----------------\n",
    "cm = confusion_matrix(y_test_enc, y_pred, labels=range(len(label_encoder.classes_)))\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Test Confusion Matrix — Topic Classification\")\n",
    "plt.xticks(rotation=17, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PREFIX}notebooks/demo_outputs/test_confusion_matrix.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
